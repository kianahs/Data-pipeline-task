{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kiana\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: mysql-connector-python in c:\\users\\kiana\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (8.0.32)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\kiana\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kiana\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kiana\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kiana\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.11.0 in c:\\users\\kiana\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mysql-connector-python) (3.17.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\kiana\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1:\n",
    "\n",
    "We call different procedures for each task in a sequence inside python and handle errors\n",
    "\n",
    "1. call import data procedures\n",
    "2. call combine data procedure\n",
    "3. call export data procedure\n",
    "\n",
    "### Solution 2:\n",
    "\n",
    "We use a single procedure to do the combine and export tasks together in the database and handle the errors with a transaction\n",
    "\n",
    "1. call import data procedures\n",
    "2. call transform and export procedure (transaction handling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)  \n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each dataframe row by row and insert it to the corresponfing table\n",
    "using stored procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_df_into_table(df, con, curs, stored_procedure_name):\n",
    "    # iterate over rows\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            # turn values to tuple to pass to the stored procedure\n",
    "            values = tuple(row)\n",
    "            # call the procedure\n",
    "            curs.callproc(stored_procedure_name, values)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # commit changes\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function we iterate over different dataframes and pass the data frame and its stored procedure name to the previous function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(data_dict, con, curs):\n",
    "  # loop on different data frames\n",
    "  for k, v in data_dict.items():\n",
    "    # extract procedure name\n",
    "    stored_procedure_name = v[1]\n",
    "    # pass it to the function to insert into corresponding table\n",
    "    insert_df_into_table(v[0], con, curs, stored_procedure_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function we call the procedure that is responsible to combine the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(con, curs, config):\n",
    "    \n",
    "    try:\n",
    "        # call the procedure defined in config file for transforming data\n",
    "        curs.callproc(config['procedures']['transform'])\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "    # commit the changes\n",
    "    con.commit()\n",
    "    \n",
    "    print(\"Data combined to data final table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function we retrieve data from the dataset using stored procedure and write it to the output file based on the defined schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_final_and_export(con, curs, config):\n",
    "    # calling the procedure for exporting the data\n",
    "   curs.callproc(config['procedures']['export'])\n",
    "   # storing the result\n",
    "   query_result = curs.stored_results()\n",
    "   # get the output directory\n",
    "   output_dir = config['output_dir']\n",
    "   # make the output directory if it doesnt exist\n",
    "   if not os.path.exists(output_dir):\n",
    "       os.makedirs(output_dir)\n",
    "   # create the output file\n",
    "   with open(os.path.join(output_dir, config['output_filename']), 'w') as f:\n",
    "            # Load schema from config\n",
    "            schema = config['schema']  \n",
    "            # iterate over query results\n",
    "            for item in query_result:\n",
    "                for row in item.fetchall():\n",
    "                    # get json data\n",
    "                    data = json.loads(row[0])\n",
    "                    # to reorder and map it to the defined schema\n",
    "                    reordered_data = OrderedDict()\n",
    "                    for field in schema:\n",
    "                        out_key = field[\"output_field\"]\n",
    "                        k = field[\"json_key\"]\n",
    "                        reordered_data[out_key] = data.get(k)\n",
    "                    # write it to the file\n",
    "                    f.write(json.dumps(reordered_data, separators=(',', ':')) + '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function get the data from the input directory defined in config file,\n",
    "put it in a dictionary (key: name, value: [dataframe, stored_procedure_name]),\n",
    "return the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_data(config):\n",
    "  # get the data\n",
    "  article_df = pd.read_csv(f'{config[\"input_data_dir\"]}/article.tsv', sep='\\t')\n",
    "  relationship_df = pd.read_csv(f'{config[\"input_data_dir\"]}/relationship.tsv', sep='\\t')\n",
    "  tag_df = pd.read_csv(f'{config[\"input_data_dir\"]}/tag.tsv', sep='\\t')\n",
    "  \n",
    "  # create data dictionary (key: name, value: [dataframe, stored_procedure_name])\n",
    "  data = {\n",
    "  'article': [article_df, config[\"procedures\"][\"import_article\"]],\n",
    "  'relationship': [relationship_df, config[\"procedures\"][\"import_relationship\"]],\n",
    "  'tag': [ tag_df, config[\"procedures\"][\"import_tag\"]]\n",
    "  }\n",
    "\n",
    "  return data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my solution 2 function which calls one procedure to do the combine and export tasks together with transaction handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_export(con, curs, config):\n",
    "   # call procedure\n",
    "   curs.callproc(config['procedures']['transform_export'])\n",
    "   # get the results\n",
    "   query_result = curs.stored_results()\n",
    "   # commit changes (since we do insertion to the data_final table)\n",
    "   con.commit() \n",
    "   # get output directory\n",
    "   output_dir = config['output_dir']\n",
    "   # if output dir doesnt exist, creates it\n",
    "   if not os.path.exists(output_dir):\n",
    "       os.makedirs(output_dir)\n",
    "   # create the output file\n",
    "   with open(os.path.join(output_dir, config['output_filename']), 'w') as f:\n",
    "        # Load schema from config\n",
    "        schema = config['schema']  \n",
    "        # iterate over query results\n",
    "        for item in query_result:\n",
    "            for row in item.fetchall():\n",
    "                # get json data\n",
    "                data = json.loads(row[0])\n",
    "                # to reorder and map it to the defined schema\n",
    "                reordered_data = OrderedDict()\n",
    "                for field in schema:\n",
    "                    out_key = field[\"output_field\"]\n",
    "                    k = field[\"json_key\"]\n",
    "                    reordered_data[out_key] = data.get(k)\n",
    "                # write it to the file\n",
    "                f.write(json.dumps(reordered_data, separators=(',', ':')) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # get config file path\n",
    "    config_file_path = 'config.json'\n",
    "    # store config parameters  \n",
    "    config = get_config(\"config.json\")\n",
    "    # get the data\n",
    "    data = get_initial_data(config)\n",
    "    \n",
    "    # create a connection to the data base using information in the config file\n",
    "    db_connection = mysql.connector.connect(host= config['host'], \n",
    "                                        user= config['user'], \n",
    "                                        password= config['password'], \n",
    "                                        database= config['database_name'], \n",
    "                                        port=config['port'])\n",
    "    \n",
    "    \n",
    "   # create a cursor\n",
    "    db_cursor = db_connection.cursor()\n",
    "    # choose the soulution \n",
    "    solution = config['solution']\n",
    "    \n",
    "    if solution == 1:\n",
    "         # in this solution we call different procedures responsible for each task in a sequence\n",
    "        try:\n",
    "            insert_data(data, db_connection, db_cursor)\n",
    "            combine_data(db_connection, db_cursor, config)\n",
    "            get_data_final_and_export(db_connection, db_cursor, config)\n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "    else:\n",
    "        # in this solution we \n",
    "        # 1: import the data with the procedure\n",
    "        # 2: Use another procedure to do the combine and export tasks together in a sequence inside the database\n",
    "        # and use transaction to handle the errors\n",
    "        try:\n",
    "            insert_data(data, db_connection, db_cursor)\n",
    "            transform_and_export(db_connection, db_cursor, config)\n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "    # closing the connection\n",
    "    db_connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "check_icv",
   "language": "python",
   "name": "check_icv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
